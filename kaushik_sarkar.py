# -*- coding: utf-8 -*-
"""Kaushik_Sarkar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ErUcBcL65gamPwJlWKWqCabFcRL11y7b

# **UNDERSTANDING DRIVING FACTORS BEHIND LOAN DEFAULT**

**Objective:** EDA to understand how consumer attributes and loan attributes influence the tendency of default.

# 1. Import libraries and set environment
"""

!python --version

#stop warnings
import warnings
warnings.filterwarnings("ignore")

#import necessary libraries
from google.colab import files  #load data files
import numpy as np              #math library
import pandas as pd             #data-table work
from datetime import datetime   #date-time library
import seaborn as sns           #data visualization with matplotlib dependency
import matplotlib.pyplot as plt
import plotly.io as pio         #interactive visualization
import plotly.express as px     #high-level plotly interface     
import re
import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk import tokenize,stem
from wordcloud import WordCloud, STOPWORDS  
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

#set row and columns display enviroment
pd.set_option("display.max_rows", None, "display.max_columns", None)
pd.set_option('display.max_colwidth', None)

"""# 2. Load & extract data"""

#load compressed data file to colab from local system
data_file = files.upload()

#load data dictionary spreadsheet to colab from local system
data_dict = files.upload()

#unzip compressed data 
!unzip loan.zip

#read data file
df = pd.read_csv("loan.csv")

"""# 3. Understand data and lay out approach to clean data"""

#check data dictionary
metadata = pd.read_excel("Data_Dictionary.xlsx")
metadata

"""**Summary description from data dictionary:**

*Loan status* is the variable of interest (dependent variable) that shows the current status of the loan. The different attributes can be categorized into consumer attributes such as employer, length of employment, home ownership and loan attributes such as interest rate, description etc. 

There are some variables which are **identifiers and not relevant** in terms of finding drivers of default. For example: id, member_id, url, 'emp_title'. 

There are other variables for which **information would be available after the loan is issued or a customer defaults**. So, these variables are not relevant for assessing risk. Examples are: 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee','recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d', 'collections_12_mths_ex_med'.  

All these columns need to be dropped for the purpose of analysis.
"""

#check first 5 rows of data
 df.head(5)

#check variable types and non-null values
df.info(verbose=True, show_counts= True)

#number of duplicate records
df.duplicated().sum()

#check number of unique values in each column
print(df.nunique())

#check variable descriptions
df.describe()

"""**Data Overview**

There are 39717 records in the data. Information are available for 110 variables apart from the variable of interest loan status. Some important insights are:

1. Loan amount varied from 500 to 35,000 with median loan amount of 10,000.
2. Funded amount and funded by investor were close to loan amount, but slightly lower. 
3. Installments varied from 16$ to a little over 1300$. Median installment was 280$. 
4. Annual income varied widely with extreme of income on the higher side.
5. Average DTI was 13 with ranges from 0 to 30.
6. Only about 15% were delinquent for two years.
7. Median months since last delinquency was 34.
8. Median open accounts were 9, with extreme values as many as 44.
9. Derogatory public records are available in very less, but as many as 4 such records are available in few.
10. Revolving balance is highly skewed. Some had nearly 150000, while majority had very low.
11. A few had public record of bankruptcies.

**Data quality issues:**


1. Missing values: Multiple clumns with no non-null value; Some columns have majority of value missing.
2. Unstandardized text: description
3. Inappropriate data type: term, interest rate, employment length, issue date
4. Possibility of disguised missing values in multiple columns: payment plan, initial list status, 12 months collections, policy code, application type, etc. have only one unique value. This may be either due to value present for one class and no value present for another class or due to only one type of class/ value.

## 4. Data cleaning
"""

#remove columns with no non-null value
df=df.dropna(axis=1,how='all')

#remove identity columns and columns for which information are available after loan issue/ charge off
df = df.drop(['id', 'member_id', 'emp_title', 'title', 'url', 'out_prncp',
              'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',
              'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',
              'recoveries', 'collection_recovery_fee', 'last_pymnt_d',
              'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d', 
              'collections_12_mths_ex_med'], axis = 1)

#identify columns with only one unique value and create a list of them
pseudo_missing_columns = df.columns[df.nunique() <= 1].to_list()

#check null values in columns having possibility of pseudo missing values
df[pseudo_missing_columns].info(verbose=True, show_counts= True)

df[pseudo_missing_columns].head(5)

"""It appears that all of these columns can be removed as the non-null unique values does not seem to represent one of two classes."""

#remove the useless columns
df = df.drop(pseudo_missing_columns, axis = 1)

#clean rows with missing values
#for columns having some missing values I will check the proportion of different classes of loan status to assess whether the presence of missing values impact the proportion of classes in loan status.
partial_missing_col = df.columns[df.isnull().any()].to_list()
for x in partial_missing_col:
  print("Distribution of loan-status categories in records having missing", x)
  print(df[~df[x].isna()].loan_status.value_counts())
  print("percentages")
  print(df[~df[x].isna()].loan_status.value_counts(normalize=True))
  print('--'*350)

"""From the above, it can be inferred that except for mths_since_last_record, removing missing values from any other column won't impact the distribution of classes in the outcome, i.e., loan status. mths_since_last_record will require imputation and imputation will be done for central tendency of each class. """

#create a list of columns based on the null records of which records will be dropped
exception_list = ["mths_since_last_record", 'mths_since_last_delinq']
for x in exception_list:
  partial_missing_col.remove(x)
print(partial_missing_col)

#create the analytical dataframe after removing the records as above
df_analytical = df.dropna(axis=0, subset=partial_missing_col)

#imputation of class wise median values 
frames = []
for i in list(set(df_analytical['loan_status'])):
    df_loan = df_analytical[df_analytical['loan_status']== i]
    df_loan['mths_since_last_record'].fillna(df_loan['mths_since_last_delinq'].median(),inplace = True)
    frames.append(df_loan)
    final_df = pd.concat(frames)

#imputation of class wise median values 
frames = []
for i in list(set(final_df['loan_status'])):
    df_loan = final_df[final_df['loan_status']== i]
    df_loan['mths_since_last_record'].fillna(df_loan['mths_since_last_record'].median(),inplace = True)
    frames.append(df_loan)
    df_final = pd.concat(frames)

df_final.info()

"""### 5. Prepare object type data & create derived columns 
####from datetime columns create year and month columns and calculate sentiment score from description
"""

#find object type columns
df_final.select_dtypes(include=['object']).columns

#create a list of object type columns
object_col_list = df_final.select_dtypes(include=['object']).columns.to_list()

#check the classes of object type columns
for x in object_col_list:
  print("No. of unique values for column", x)
  print(df_final[x].nunique())
  print("Unique values for column", x)
  print(df_final[x].unique())
  print('--'*350)

#convert to float type
df_final['int_rate'] = df_final['int_rate'].map(lambda x: float(x.strip('%'))/100)
df_final['revol_util'] = df_final['revol_util'].map(lambda x: float(x.strip('%'))/100)

#convert to month year columns to mm-dd-yyyy format
df_final['issue_d']= df_final['issue_d'].map(lambda x: datetime.strftime(datetime.strptime(x, '%b-%y'), '%m/%d/%Y'))
df_final['earliest_cr_line']= df_final['earliest_cr_line'].map(lambda x: datetime.strftime(datetime.strptime(x, '%b-%y'), '%m/%d/%Y'))

#final convert to datetime data type
#point to note is that this will change the column data formats to yyyy-mm-dd
df_final['issue_d'] = pd.to_datetime(df_final['issue_d'])
df_final['earliest_cr_line'] = pd.to_datetime(df_final['earliest_cr_line'])

#create month and year columns from datetime columns and convert year columns to string
df_final['issue_month'] = df_final['issue_d'].dt.month_name()
df_final['ecr_month'] = df_final['earliest_cr_line'].dt.month_name()

df_final['issue_y'] = df_final['issue_d'].dt.year.apply(str)
df_final['ecr_y'] = df_final['earliest_cr_line'].dt.year.apply(str)

#standardize text of description [remove borrower added on text and special character and dates]
df_final.desc.replace(
    {'Borrower added on ': '',
     '>': '',
     '<': '',
     '\d+[\/:\-]\d+[\/:\-\s]*[\dAaPpMm]*' : '',
     '\w+\s\d+[\,]\s\d+' : '',
     'br': ''},
    regex=True,
    inplace=True,
    )

#create a function for lemmatization and stemming and removal of residual non alphabets
def nlp_preprocesser(row):
    sentence = row.desc
    #convert all characters to lowercase
    lowered = sentence.lower()
    tok = tokenize.word_tokenize(lowered)

    #lemmatizing & stemming
    lemmatizer = stem.WordNetLemmatizer()
    lem = [lemmatizer.lemmatize(i) for i in tok if i not in STOPWORDS]
    stemmer = stem.PorterStemmer()
    stems = [stemmer.stem(i) for i in lem if i not in STOPWORDS]

    #remove non-alphabetical characters like '(', '.' or '!'
    alphas = [i for i in stems if i.isalpha() and (i not in STOPWORDS)]
    return " ".join(alphas)

#process the description column with the function
df_final['Description_processed'] = df_final.apply(nlp_preprocesser, axis=1)

df_final['emp_length'] = df_final['emp_length'].str.replace('< 1 year', str(0))
df_final['emp_length'] = df_final['emp_length'].str.replace('\+ years', '')
df_final['emp_length'] = df_final['emp_length'].str.replace(' years', '')
df_final['emp_length'] = df_final['emp_length'].str.replace(' year', '')
df_final['emp_length'] = pd.to_numeric(df_final['emp_length'])
df_final['emp_length'].fillna(value = 0, inplace = True)

df_final['desc_length'] = df_final['Description_processed'].apply(lambda x: len(x))

def sentiment_score(text):
    analyzer = SentimentIntensityAnalyzer()
    sent_score = analyzer.polarity_scores(text)["compound"]
    return float(sent_score)

#create sentiment score column from descriptions
df_final['sentiment_score'] = df_final['Description_processed'].apply(lambda x: sentiment_score(x))

df_final = df_final.drop(["Description_processed", "desc"], axis = 1)

"""#5. Prepare numerical columns"""

#create a list of numerical columns present in the subset 
num_col = df_final.select_dtypes(exclude=['object']).columns.to_list()

#find number of unique values in numerical columns
for x in num_col:
  print("No. of unique values for column-", x, ":", df_final[x].nunique())

"""There are multiple numerical columns having less than 5 numerical values. It will be checked whether they can be treated as categorical columns."""

#find columns with less than 5 unique values
for x in num_col:
  if df_final[x].nunique() <5:
    print(x)
    print(df_final[x].unique())

df_final['pub_rec']= df_final['pub_rec'].apply(str)
df_final['pub_rec_bankruptcies']= df_final['pub_rec_bankruptcies'].apply(str)

"""##6. Univariate and Segmented Univariate analysis """

#five number summary of quantitative variables
df_final.describe()

#summary of object type data (number of classes, most frequent class, and its frequency)
df_final.describe(include=object)

"""##6.1 Visualize loan status class proportions"""

#proportion of loan status categories
total_records = float(len(df_final))
ax = sns.countplot(x="loan_status", data=df_final, palette="Set3")
plt.title('Fig. 1. Loan Status', fontsize=14)
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height()/total_records)
    x = p.get_x() + p.get_width()
    y = p.get_height()
    ax.annotate(percentage, (x, y),ha='right')

"""##6.2 visualize distribution of object columns"""

#create a list of variables for countplot
countplot_col = []
for col in df_final.columns:
  if col not in num_col:
    if df_final[col].nunique() < 50:
      countplot_col.append(col)
#create a dataframe having columns to be used for countplot
df_obj = df_final[countplot_col]
#create countplots
for i, col in enumerate(df_obj.columns):
    plt.figure(i)
    figsize = (50, 10)
    plt.figure(figsize=figsize)
    sns.countplot(x=col, data=df_obj)

"""**Majority of loans have been given to/ for:**

36 months
Lending club A and B grades
A4, A5, B3, B4, B5 subgrades
Persons living in rent or mortgage
unverified income source
Individuals living in California


**Other insights**
Most of the loans have been fully paid. Charged off loans are more than current 
loans.
Most of the loans were taken for debt consolidation, followed by credit card, small business, and home improvement.
Most of the loans data were from loans issued towards the winter and in 2011 and 2010.
Earliest credit line goes back to 1971.

##6.3 visualize distribution of numerical columns
"""

# creating a list of numerical columns present in the subset 
num_col = df_final.select_dtypes(exclude=['object', 'datetime']).columns.to_list() 
len(num_col)

# Plotting the distribution of numerical columns in the dataframe
fig, ax = plt.subplots(2, 10, figsize=(40, 15))
fig.suptitle('Fig.2. Distribution of Numerical Features', fontsize=26)
sns.set_style("whitegrid")
for variable, subplot in zip(num_col, ax.flatten()):
    sns.distplot(df_final[variable], bins = 20, ax=subplot)
    for label in subplot.get_xticklabels():
        label.set_rotation(90);

#measure skewness
skewness = df_final[num_col].skew(axis = 0).to_frame("Skewness")
skewness.style.applymap(lambda x: 'background-color : red' if x>1 or x<-1 else ('background-color : orange' if x>0.5 or x<-0.5 else ''))

#create two lists based on skewness
num_col = df_final.select_dtypes(exclude=['object', 'datetime']).columns.to_list() 
num_col_1 = []
num_col_2 = []

for col in num_col:
  if df_final[col].skew()< -.5 or df_final[col].skew()> .5:
    num_col_1.append(col)
  else:
    num_col_2.append(col)

#create boxplot for numerical columns highly skewed
figsize = (50, 15)
plt.figure(figsize=figsize)

plt.title('Fig.3a. Boxplot of Numerical Features', fontsize=26)
df_quant = df_final[num_col_1]
df_melted = pd.melt(df_quant)
sns.boxplot(x='value', y='variable', orient="h", data=df_melted);

#create boxplot for numerical columns not highly skewed
figsize = (50, 10)
plt.figure(figsize=figsize)

plt.title('Fig.3b. Boxplot of Numerical Features', fontsize=26)
df_quant = df_final[num_col_2]
df_melted = pd.melt(df_quant)
sns.boxplot(x='value', y='variable', orient="h", data=df_melted);

"""Insights

Data descriptions for critical quantitative variables are provided before. Several variables including annual income, loan amount, interest rate are highly skewed.

##7.Bivariate Analysis and Multivariable Analysis
"""

obj_col = df_final.select_dtypes(exclude=['int64', 'float64', 'datetime']).columns.to_list() 
len(obj_col)

df_obj = df_final[obj_col]

df_obj.columns

for i, col in enumerate(df_obj.columns):
    plt.figure(i)
    figsize = (50, 10)
    plt.figure(figsize=figsize)
    sns.countplot(x=col, hue="loan_status", data=df_obj)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='loan_amnt', data=df_final, palette='rainbow')
plt.title("Distribution of loan amount by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='int_rate', data=df_final, palette='rainbow')
plt.title("Distribution of interest rate by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='installment', data=df_final, palette='rainbow')
plt.title("Distribution of installments by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='emp_length', data=df_final, palette='rainbow')
plt.title("Distribution of employment length by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='annual_inc', data=df_final, palette='rainbow')
plt.title("Distribution of annual income by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='dti', data=df_final, palette='rainbow')
plt.title("Distribution of DTI by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='delinq_2yrs', data=df_final, palette='rainbow')
plt.title("Distribution of 2Y delinquency by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='inq_last_6mths', data=df_final, palette='rainbow')
plt.title("Distribution of number of enquiries in last 6 months by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='mths_since_last_delinq', data=df_final, palette='rainbow')
plt.title("Distribution of months since last delinquency by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='mths_since_last_record', data=df_final, palette='rainbow')
plt.title("Distribution of months since last record by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='open_acc', data=df_final, palette='rainbow')
plt.title("Distribution of open accounts by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='revol_bal', data=df_final, palette='rainbow')
plt.title("Distribution of revolving balance by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='revol_util', data=df_final, palette='rainbow')
plt.title("Distribution of revolving utilization by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='total_acc', data=df_final, palette='rainbow')
plt.title("Distribution of total accounts by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='desc_length', data=df_final, palette='rainbow')
plt.title("Distribution of loan description length by loan status", fontsize=26)

plt.figure(figsize=(20,10))
sns.boxenplot(x='loan_status', y='sentiment_score', data=df_final, palette='rainbow')
plt.title("Distribution of sentiment score by loan status", fontsize=26)

df_ts1 = (df_final.reset_index().groupby(['issue_d','loan_status'], as_index=False).count().rename(columns={'index':'ct'}))
df_ts2 = (df_final.reset_index().groupby(['earliest_cr_line','loan_status'], as_index=False).count().rename(columns={'index':'ct'}))

fig, ax = plt.subplots()
for key, data in df_ts1.groupby('loan_status'):
    data.plot(x='issue_d', y='ct', ax=ax, label=key)

fig, ax = plt.subplots()
for key, data in df_ts2.groupby('loan_status'):
    data.plot(x='earliest_cr_line', y='ct', ax=ax, label=key)

"""**Insights**

Chances of charged off loans are apparently higher with:

1. 60 month tenure
2. Loan grade E and F
3. Rent more than mortgage
4. Source verified income
5. 1 derogatory public record
6. More recent ECRs
7. Higher interest rates
8. More enquiries in last 6 months
9. Lesser description sentiment score
10. More revolving utilization 
11. More DTI

##8.Complex multivariable relationships
"""

sns.pairplot(df_final, hue="loan_status", markers=["o", "s", "D"], corner= True)
plt.title("Pairwise relation of variables by loan status", fontsize=26);

fig = px.box(df_final, x="annual_inc", y="loan_amnt", color="loan_status",
             notched=True, # used notched shape
             title="How loan amount and annual income impacts loan status",
             hover_data=["issue_month"] # add day column to hover data
            )
fig.show()

fig = px.box(df_final, x="int_rate", y="annual_inc", color="loan_status",
             notched=True, # used notched shape
             title="How annual income and interest rates impacts loan status",
             hover_data=["issue_month"] # add day column to hover data
            )
fig.show()

fig = px.box(df_final, x="revol_bal", y="inq_last_6mths", color="loan_status",
             notched=True, # used notched shape
             title="How loan enquiries and revolving balance impact loan status",
             hover_data=["issue_month"] # add day column to hover data
            )
fig.show()

fig = px.box(df_final, x="total_acc", y="open_acc", color="loan_status",
             notched=True, # used notched shape
             title="How total vs open accounts impact loan status",
             hover_data=["issue_month"] # add day column to hover data
            )
fig.show()

fig = px.box(df_final, x="sentiment_score", y="desc_length", color="loan_status",
             notched=True, # used notched shape
             title="How description length and its sentiment reflects the loan status risk",
             hover_data=["issue_month"] # add day column to hover data
            )
fig.show()

fig = px.treemap(df_final, path=[px.Constant("Loan Status"), 'loan_status', 'issue_y', 'term', 'emp_length'], values='loan_amnt',
                  color='annual_inc', hover_data=['addr_state'],
                  color_continuous_scale='RdBu',
                  color_continuous_midpoint=np.average(df_final['annual_inc'], weights=df_final['loan_amnt']))
fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))
fig.show()

"""1. Lower annual income irrespective of loan amount, but with higher interest rates have greater frequency of charge off.
2. More inquiries in last 6 months with less revolving balance is more likely to get into charge off.
3. The extreme positive of sentiment score of description along with large description may result in more likely charge off. 
4. Charge off is more with higher number of total accounts when open accounts are more.
5. Income less than 80K along with higher employment length and loan tenure of 60 months are more likely to charge off.

"""